{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1669119a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e19565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# CLIP Setup\n",
    "# -----------------------------\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fb35da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "def embedd_image(image_data):\n",
    "    if isinstance(image_data,str): # if image path is given thi execute\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_data\n",
    "    inputs = clip_processor(images=image,return_tensors=\"pt\")\n",
    "    with torch.no_grad(): #normalizing image to unitvector\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features = features/features.norm(dim=1,keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "def embedd_text(text):\n",
    "    inputs = clip_processor(text=text,padding=True,return_tensors=\"pt\",truncation=True,max_length=77)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        features = features/features.norm(dim=1,keepdim=True)\n",
    "        return features.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_images(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    data = []\n",
    "\n",
    "    for i, page in enumerate(doc, start=1):\n",
    "        # Text extraction\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            data.append({\n",
    "                \"type\": \"text\",\n",
    "                \"page\": i,\n",
    "                \"content\": text})\n",
    "\n",
    "        # Image extraction\n",
    "        image = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(image):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            img_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "            data.append({\n",
    "                \"type\": \"image\",\n",
    "                \"page\": i,\n",
    "                \"content\": img_base64\n",
    "            })\n",
    "\n",
    "    doc.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# STEP 2 — Convert to LangChain Documents\n",
    "# -----------------------------\n",
    "def convert_to_documents(data):\n",
    "    docs = []\n",
    "    for item in data:\n",
    "        if item[\"type\"] == \"text\":\n",
    "            docs.append(Document(\n",
    "                page_content=item[\"content\"],\n",
    "                metadata={\"type\": \"text\",\n",
    "                           \"page\": item[\"page\"]}\n",
    "            ))\n",
    "        else:\n",
    "            docs.append(Document(\n",
    "                page_content=\"[IMAGE DATA]\",\n",
    "                metadata={\"type\": \"image\",\n",
    "                           \"page\": item[\"page\"],\n",
    "                            \"b64\": item[\"content\"]}\n",
    "            ))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26b49642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 3 — Create multimodal embeddings\n",
    "# -----------------------------\n",
    "class MultiModalEmbeddings:\n",
    "    def __init__(self):\n",
    "        pass  # CLIP is already loaded globally\n",
    "\n",
    "    def embed_documents(self, docs):\n",
    "        embeddings = []\n",
    "        for doc in docs:\n",
    "            if doc.metadata[\"type\"] == \"text\":\n",
    "                emb = embedd_text(doc.page_content)\n",
    "            elif doc.metadata[\"type\"] == \"image\":\n",
    "                img_data = base64.b64decode(doc.metadata[\"b64\"])\n",
    "                image = Image.open(BytesIO(img_data)).convert(\"RGB\")\n",
    "                emb = embedd_image(image)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown doc type: {doc.metadata['type']}\")\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        if isinstance(query, str):\n",
    "            return embedd_text(query)\n",
    "        elif isinstance(query, Image.Image):\n",
    "            return embedd_image(query)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported query type for embedding\")\n",
    "        \n",
    "    def __call__(self, text: str):\n",
    "        return self.embed_query(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32433285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# STEP 4 — Create Vector Store\n",
    "# -----------------------------\n",
    "def create_vectorstore(docs):\n",
    "    embedder = MultiModalEmbeddings()\n",
    "    embeddings = embedder.embed_documents(docs) #calling embed_document methood from MultieModalEmbidding class\n",
    "\n",
    "    # All vectors now same dimension from CLIP\n",
    "    text_embeddings = [(doc.page_content, emb) for doc, emb in zip(docs, embeddings)]\n",
    "\n",
    "    vectorstore = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embeddings,\n",
    "    embedding=embedder,  # So it can embed queries\n",
    "    metadatas=[doc.metadata for doc in docs]\n",
    "            )\n",
    "    return vectorstore, embedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbdc78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# STEP 5 — RetrievalQA Pipeline\n",
    "# -----------------------------\n",
    "def build_rag_pipeline(vectorstore):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)  # or \"gpt-4o-mini\" for cheaper\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbaf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I'm unable to display images, but I can explain the content based on the text provided. The document summarizes revenue trends across the first three quarters of the year. It indicates that revenue grew steadily, with the highest growth recorded in Q3. In Q1, there was a moderate increase in revenue due to the introduction of new product lines. Q2 saw better performance than Q1, attributed to marketing campaigns. Q3 experienced exponential growth, primarily due to global expansion.\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "\n",
      "[Page 1] TEXT:\n",
      "Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\n",
      "below, revenue grew steadily with the highest growth recorded in Q3.\n",
      "Q1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\n",
      "Q1 due to marketing campaigns. Q3 had exponential growth due to global expansion.\n",
      "\n",
      "\n",
      "[Page 1] IMAGE:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "\n",
    "pdf_path = \"multimodal_sample.pdf\"\n",
    "extracted_data = extract_text_and_images(pdf_path) #Extract text and Images from PDF\n",
    "docs = convert_to_documents(extracted_data)  #convert to langchian Document\n",
    "vs, embedder = create_vectorstore(docs) #Create vector store \n",
    "qa = build_rag_pipeline(vs)\n",
    "\n",
    "query = \"Sshow the image in document and explain it\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\n--- Retrieved Documents ---\\n\")\n",
    "\n",
    "for doc in result[\"source_documents\"]:\n",
    "    if doc.metadata[\"type\"] == \"text\":\n",
    "        print(f\"[Page {doc.metadata['page']}] TEXT:\\n{doc.page_content}\\n\")\n",
    "    elif doc.metadata[\"type\"] == \"image\":\n",
    "        print(f\"[Page {doc.metadata['page']}] IMAGE:\")\n",
    "        img_data = base64.b64decode(doc.metadata[\"b64\"])\n",
    "        display(IPImage(data=img_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33dd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f3a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
